[audio]
sampling_rate = 16000
segment_length = 16384
channels = 2
normalize = True
dtype = float32

[dataset]
datapath = D:\kelse\02_Media\AudioDatasets\KC_SoundLibrary
test_dataset = test_audio
streaming = True
max_files = 10
shuffle = True
hop_size = 8192
num_workers = 0
check_audio = True
check_dataset = True

[model]
net_type = UNetV0
in_channels = 2
channels = [8, 32, 64]
factors = [1, 4, 4]
items = [1, 2, 2]
attentions = [0, 0, 1]
attention_heads = 4
attention_features = 32
diffusion_type = VDiffusion
sampler_type = VSampler
loss_function = mse_loss

[training]
epochs = 20
learning_rate = 0.0001
batch_size = 2
optimizer = AdamW
device = cuda:0
save_every = 5
sample_every = 2
max_batches_per_epoch = 20
gradient_clip = 1.0

[paths]
workspace = D:\kelse\03_Repositories\Kelsey_audio-diffusion-pytorch
checkpoint_dir = {timestamp}_small_test
training_audio_dir = training_audio

[monitoring]
generate_samples = True
num_samples_per_epoch = 2
sample_steps = 5
save_best_model = True
track_loss = True
progress_bar = True

[notes]
additional_notes = Small test config for quick experimentation - segment_length=16384 samples (~1.02s at 16kHz) - hop_size=8192 (50% overlap) - batch_size=2 (minimal) - max_batches_per_epoch=20 - max_files=10 - Model=[8,32,64] (~300K params) - Quick iteration to verify pipeline works

[extra]
description = 20250113_audio-diffusion-kelsey-small-test
start_time = 
end_time = 
time_elapsed = 
best_loss = 
total_samples_generated = 
gpu_memory_used = 
notes_during_training = 

[resume]
resume_from_checkpoint = False
checkpoint_path = 
start_epoch = 0 