[audio]
sampling_rate = 16000
segment_length = 32768
channels = 2
normalize = True
dtype = float32

[dataset]
datapath = D:\kelse\03_Repositories\Kelsey_audio-diffusion-pytorch\test_audio
test_dataset = test_audio
streaming = True
max_files = 
shuffle = True
hop_size = 8192
num_workers = 0
check_audio = True
check_dataset = True

[model]
net_type = UNetV0
in_channels = 2
channels = [8, 32, 64, 128, 256]
factors = [1, 4, 4, 4, 2]
items = [1, 2, 2, 2, 2]
attentions = [0, 0, 0, 1, 1]
attention_heads = 8
attention_features = 64
diffusion_type = VDiffusion
sampler_type = VSampler
loss_function = mse_loss

[training]
epochs = 100
learning_rate = 0.0001
batch_size = 4
optimizer = AdamW
device = cuda:0
save_every = 10
sample_every = 5
max_batches_per_epoch = 100
gradient_clip = 1.0

[paths]
workspace = D:\kelse\03_Repositories\Kelsey_audio-diffusion-pytorch
checkpoint_dir = {timestamp}_{experiment}_checkpoints
training_audio_dir = training_audio

[monitoring]
generate_samples = True
num_samples_per_epoch = 3
sample_steps = 10
save_best_model = True
track_loss = True
progress_bar = True

[notes]
additional_notes = Diffusion model training for audio generation - segment_length=32768 samples (~2.05s at 16kHz) - hop_size=16384 (50% overlap) - batch_size=4 (start small) - max_batches_per_epoch=100 - Model=[8,32,64,128,256] (~6.8M params) - Strategy: Start small, monitor samples every 5 epochs, increase size when stable

[extra]
description = 20250903_audio-diffusion-kelsey-unconditional-training
start_time = 
end_time = 
time_elapsed = 
best_loss = 
total_samples_generated = 
gpu_memory_used = 
notes_during_training = 

[resume]
resume_from_checkpoint = 
checkpoint_path = 
start_epoch = 0 