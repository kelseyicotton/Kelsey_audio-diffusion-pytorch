[audio]
sampling_rate = 16000
segment_length = 32768
channels = 2
normalize = True
dtype = float32

[dataset]
datapath = training_audio
test_dataset = test_audio
streaming = True
max_files = 100
shuffle = True
hop_size = 8192
num_workers = 0
check_audio = True
check_dataset = True

[model]
net_type = UNetV0
in_channels = 2
channels = [8, 32, 64, 128, 256]
factors = [1, 4, 4, 4, 2]
items = [1, 2, 2, 2, 2]
attentions = [0, 0, 0, 1, 1]
attention_heads = 4
attention_features = 32
diffusion_type = VDiffusion
sampler_type = VSampler
loss_function = mse_loss

[training]
epochs = 201
learning_rate = 0.00005
batch_size = 4
optimizer = AdamW
device = cuda:0
save_every = 2
sample_every = 10
max_batches_per_epoch = 50
gradient_clip = 0.5

[paths]
workspace = D:\kelse\03_Repositories\Kelsey_audio-diffusion-pytorch
checkpoint_dir = {timestamp}_local_test

[monitoring]
generate_samples = True
num_samples_per_epoch = 2
sample_steps = 150
save_best_model = True
track_loss = True
progress_bar = True

[notes]
additional_notes = Local test with synthetic audio files - segment_length=32768 samples (~2.05s at 16kHz) - 86 clean pattern files - Larger model (6.8M params) for better pattern learning - Extended training (200 epochs)

[extra]
description = 20250903_audio-diffusion-kelsey-local-test-synthetic
start_time = 
end_time = 
time_elapsed = 
best_loss = 
total_samples_generated = 
gpu_memory_used = 
notes_during_training = 

[resume]
resume_from_checkpoint = False
checkpoint_path = 
start_epoch = 0
